{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('ml_course': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "9e6836700e96a853fe4b0f9f6772835270a845f6671132c1dde5eaf8914ce878"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1. Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_file_1 = Path(os.path.abspath(\"\")) / \"data\" / \"ex2data1.txt\"\n",
    "data_set_1 = pd.read_csv(data_file_1, header=None, names=[\"Score_1\", \"Score_2\", \"Admitted\"])\n",
    "# print(data_set_1)\n",
    "# print(data_set_1.head())\n",
    "# print(data_set_1.describe())\n",
    "\n",
    "x = np.array(data_set_1.iloc[:, 0:2])\n",
    "y = np.array(data_set_1.iloc[:, 2:])\n",
    "m, n = x.shape\n",
    "print(m, n)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "source": [
    "## 1.1 Visualizing the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init empty array with dimensions\n",
    "admitted_list = np.empty((0, 3))\n",
    "non_admitted_list = np.empty((0, 3))\n",
    "\n",
    "for record in np.array(data_set_1):\n",
    "    if record[2] == 1:\n",
    "        admitted_list = np.append(admitted_list, [record], axis=0)\n",
    "    else:\n",
    "        non_admitted_list = np.append(non_admitted_list, [record], axis=0)\n",
    "\n",
    "# simple way to init\n",
    "# admitted_list = np.array([record for record in np.array(data_set_1) if record[2] == 1])\n",
    "# non_admitted_list = np.array([record for record in np.array(data_set_1) if record[2] == 0])\n",
    "\n",
    "# print(admitted_list)\n",
    "# print(not_admitted_list)\n",
    "\n",
    "plt.plot(admitted_list[:, 0], admitted_list[:, 1], 'go', label=\"Admitted\")\n",
    "plt.plot(non_admitted_list[:, 0], non_admitted_list[:, 1], 'ro', label=\"Non-admitted\")\n",
    "plt.xlabel(\"Score_1\")\n",
    "plt.ylabel(\"Score_2\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "source": [
    "## 1.2 Implementation\n",
    "### 1.2.1 Sigmoid function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# draw sigmoid\n",
    "sigmoid_x = np.linspace(-10, 10)\n",
    "sigmoid_y = sigmoid(sigmoid_x)\n",
    "plt.plot(sigmoid_x, sigmoid_y)\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"sigmoid(z)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Cost function and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.insert(x, 0, np.ones(m), axis=1)\n",
    "# print(X)\n",
    "init_theta = np.zeros((n + 1, 1))\n",
    "# print(init_theta)\n",
    "\n",
    "def cost(theta, X, y):\n",
    "    h = sigmoid(X @ theta)\n",
    "    m = X.shape[0]\n",
    "    j = 1 / m * (-y.T @ np.log(h) - (1 - y).T @ np.log(1 - h))\n",
    "    return j.item()\n",
    "\n",
    "def gradient(theta, X, y):\n",
    "    m = X.shape[0]\n",
    "    gradient = 1 / m * X.T @ (sigmoid(X @ theta) - y)\n",
    "    return gradient\n",
    "\n",
    "# cost and gradient when theta = [[0], [0], [0]]\n",
    "print(cost(init_theta, X, y))\n",
    "print(gradient(init_theta, X, y))\n",
    "\n",
    "# cost and gradient when theta = [[-24], [0.2], [0.2]]\n",
    "print(cost(np.matrix([[-24], [0.2], [0.2]]), X, y))\n",
    "print(gradient(np.matrix([[-24], [0.2], [0.2]]), X, y))"
   ]
  },
  {
   "source": [
    "### 1.2.3 Learning parameters using library"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use TNC algorithm to find best theta\n",
    "# notice that y need to be 1-d array when using this method\n",
    "res = opt.minimize(fun = cost, x0 = init_theta, args = (X, y.flatten()), method = 'TNC', jac = gradient)\n",
    "learned_theta = res.x\n",
    "print(res)\n",
    "print(learned_theta)\n",
    "\n",
    "# draw decision boundary\n",
    "res_x1 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
    "res_x2 = (0 - learned_theta[0] - learned_theta[1] * res_x1) / learned_theta[2]\n",
    "print(res_x1)\n",
    "print(res_x2)\n",
    "\n",
    "plt.plot(admitted_list[:, 0], admitted_list[:, 1], 'go', label=\"Admitted\")\n",
    "plt.plot(non_admitted_list[:, 0], non_admitted_list[:, 1], 'ro', label=\"Non-admitted\")\n",
    "plt.plot(res_x1, res_x2, \"-\")\n",
    "plt.xlabel(\"Score_1\")\n",
    "plt.ylabel(\"Score_2\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### 1.2.4 Evaluating logistic regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(theta, x):\n",
    "    return sigmoid(theta.T @ x).item()\n",
    "\n",
    "print(\"probability: {}\".format(hypothesis(learned_theta, np.array([[1], [45], [85]]))))\n",
    "\n",
    "def predict(theta, X):\n",
    "    p = sigmoid(X @ theta)\n",
    "    res = np.array([1 if record >= 0.5 else 0 for record in p])\n",
    "    return res\n",
    "\n",
    "predict_res = predict(learned_theta, X)\n",
    "predict_accuracy = np.sum([1 if predict_res[i] == y[i][0] else 0 for i in range(len(y))]) / len(y)\n",
    "print(\"accuracy on training data: {}\".format(predict_accuracy))"
   ]
  }
 ]
}