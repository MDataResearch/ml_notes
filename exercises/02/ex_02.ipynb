{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('ml_course': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "9e6836700e96a853fe4b0f9f6772835270a845f6671132c1dde5eaf8914ce878"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1. Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_file_1 = Path(os.path.abspath(\"\")) / \"data\" / \"ex2data1.txt\"\n",
    "data_set_1 = pd.read_csv(data_file_1, header=None, names=[\"Score_1\", \"Score_2\", \"Admitted\"])\n",
    "# print(data_set_1)\n",
    "# print(data_set_1.head())\n",
    "# print(data_set_1.describe())\n",
    "\n",
    "x = np.array(data_set_1.iloc[:, 0:2])\n",
    "y = np.array(data_set_1.iloc[:, 2:])\n",
    "m, n = x.shape\n",
    "print(m, n)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "source": [
    "## 1.1 Visualizing the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init empty array with dimensions\n",
    "admitted_list = np.empty((0, 3))\n",
    "non_admitted_list = np.empty((0, 3))\n",
    "\n",
    "for record in np.array(data_set_1):\n",
    "    if record[2] == 1:\n",
    "        admitted_list = np.append(admitted_list, [record], axis=0)\n",
    "    else:\n",
    "        non_admitted_list = np.append(non_admitted_list, [record], axis=0)\n",
    "\n",
    "# simple way to init\n",
    "# admitted_list = np.array([record for record in np.array(data_set_1) if record[2] == 1])\n",
    "# non_admitted_list = np.array([record for record in np.array(data_set_1) if record[2] == 0])\n",
    "\n",
    "# print(admitted_list)\n",
    "# print(non_admitted_list)\n",
    "\n",
    "plt.plot(admitted_list[:, 0], admitted_list[:, 1], 'go', label=\"Admitted\")\n",
    "plt.plot(non_admitted_list[:, 0], non_admitted_list[:, 1], 'ro', label=\"Non-admitted\")\n",
    "plt.xlabel(\"Score_1\")\n",
    "plt.ylabel(\"Score_2\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "source": [
    "## 1.2 Implementation\n",
    "### 1.2.1 Sigmoid function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# draw sigmoid\n",
    "sigmoid_x = np.linspace(-10, 10)\n",
    "sigmoid_y = sigmoid(sigmoid_x)\n",
    "plt.plot(sigmoid_x, sigmoid_y)\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"sigmoid(z)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Cost function and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.insert(x, 0, np.ones(m), axis=1)\n",
    "# print(X)\n",
    "init_theta = np.zeros((n + 1, 1))\n",
    "# print(init_theta)\n",
    "\n",
    "def cost(theta, X, y):\n",
    "    h = sigmoid(X @ theta)\n",
    "    m = X.shape[0]\n",
    "    j = 1 / m * (-y.T @ np.log(h) - (1 - y).T @ np.log(1 - h))\n",
    "    return j.item()\n",
    "\n",
    "def gradient(theta, X, y):\n",
    "    m = X.shape[0]\n",
    "    gradient = 1 / m * X.T @ (sigmoid(X @ theta) - y)\n",
    "    return gradient\n",
    "\n",
    "# cost and gradient when theta = [[0], [0], [0]]\n",
    "print(cost(init_theta, X, y))\n",
    "print(gradient(init_theta, X, y))\n",
    "\n",
    "# cost and gradient when theta = [[-24], [0.2], [0.2]]\n",
    "print(cost(np.matrix([[-24], [0.2], [0.2]]), X, y))\n",
    "print(gradient(np.matrix([[-24], [0.2], [0.2]]), X, y))"
   ]
  },
  {
   "source": [
    "### 1.2.3 Learning parameters using library"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use TNC algorithm to find best theta\n",
    "# notice that y need to be 1-d array when using this method\n",
    "res = opt.minimize(fun = cost, x0 = init_theta, args = (X, y.flatten()), method = 'TNC', jac = gradient)\n",
    "learned_theta = res.x\n",
    "print(res)\n",
    "print(learned_theta)\n",
    "\n",
    "# draw decision boundary\n",
    "res_x1 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
    "res_x2 = (0 - learned_theta[0] - learned_theta[1] * res_x1) / learned_theta[2]\n",
    "print(res_x1)\n",
    "print(res_x2)\n",
    "\n",
    "plt.plot(admitted_list[:, 0], admitted_list[:, 1], 'go', label=\"Admitted\")\n",
    "plt.plot(non_admitted_list[:, 0], non_admitted_list[:, 1], 'ro', label=\"Non-admitted\")\n",
    "plt.plot(res_x1, res_x2, \"-\")\n",
    "plt.xlabel(\"Score_1\")\n",
    "plt.ylabel(\"Score_2\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### 1.2.4 Evaluating logistic regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(theta, x):\n",
    "    return sigmoid(theta.T @ x).item()\n",
    "\n",
    "print(\"probability: {}\".format(hypothesis(learned_theta, np.array([[1], [45], [85]]))))\n",
    "\n",
    "def predict(theta, X):\n",
    "    p = sigmoid(X @ theta)\n",
    "    res = np.array([1 if record >= 0.5 else 0 for record in p])\n",
    "    return res\n",
    "\n",
    "predict_res = predict(learned_theta, X)\n",
    "predict_accuracy = np.sum([1 if predict_res[i] == y[i][0] else 0 for i in range(len(y))]) / len(y)\n",
    "print(\"accuracy on training data: {}\".format(predict_accuracy))"
   ]
  },
  {
   "source": [
    "# 2. Regularized logistic regression\n",
    "## 2.1 Visualizing the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_2 = Path(os.path.abspath(\"\")).absolute() / \"data\" / \"ex2data2.txt\"\n",
    "data_set_2 = pd.read_csv(data_file_2, header=None, names=[\"Test_1\", \"Test_2\", \"Accepted\"])\n",
    "# print(data_set_1)\n",
    "# print(data_set_1.head())\n",
    "# print(data_set_1.describe())\n",
    "\n",
    "x = np.array(data_set_2.iloc[:, 0:2])\n",
    "y = np.array(data_set_2.iloc[:, 2:])\n",
    "m, n = x.shape\n",
    "# print(m, n)\n",
    "# print(x)\n",
    "# print(y)\n",
    "\n",
    "accepted_list = np.array([record for record in np.array(data_set_2) if record[2] == 1])\n",
    "rejected_list = np.array([record for record in np.array(data_set_2) if record[2] == 0])\n",
    "\n",
    "print(accepted_list)\n",
    "print(rejected_list)\n",
    "\n",
    "plt.plot(accepted_list[:, 0], accepted_list[:, 1], 'go', label=\"Accepted\")\n",
    "plt.plot(rejected_list[:, 0], rejected_list[:, 1], 'ro', label=\"Rejected\")\n",
    "plt.xlabel(\"Test_1\")\n",
    "plt.ylabel(\"Test_2\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## 2.2 Feature mapping"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# map (x1, x2) to \n",
    "# (x1, x2, x1^2, x1 x2, x2^2, x1^3, x1^2 x2, x1 x2^2, x2^3, ..., x1 x2^5, x2^6) \n",
    "# then insert x0\n",
    "def mapFeature(x):\n",
    "    X = np.copy(x)\n",
    "    # -1 means unknown dimension\n",
    "    x1 = x[:, 0].reshape((-1, 1))\n",
    "    x2 = x[:, 1].reshape((-1, 1))\n",
    "    powers = [i for i in range(2, 7)]\n",
    "    for power in powers:\n",
    "        x1_powers = [i for i in range(0, power + 1)][::-1]\n",
    "        for x1_power in x1_powers:\n",
    "            x2_power = power - x1_power\n",
    "            new_column = x1 ** x1_power * x2 ** x2_power\n",
    "            X = np.append(X, new_column, axis=1)\n",
    "    X = np.insert(X, 0, np.ones(X.shape[0]), axis=1)\n",
    "    return X\n",
    "\n",
    "X = mapFeature(x)\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "source": [
    "## 2.3 Cost function and gradient"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_cost(theta, lamda, X, y):\n",
    "    m = X.shape[0]\n",
    "    regular_theta = np.insert(theta[1:], 0, 0, axis=0)\n",
    "    logistic_cost = cost(theta, X, y)\n",
    "    regularize_factor = lamda / (2 * m) * (regular_theta.T @ regular_theta).item()\n",
    "    return logistic_cost + regularize_factor\n",
    "\n",
    "def regularized_gradient(theta, lamda, X, y):\n",
    "    m = X.shape[0]\n",
    "    regular_theta = np.insert(theta[1:], 0, 0, axis=0)\n",
    "    logistic_gradient = gradient(theta, X, y)\n",
    "    regularize_vector = lamda / m * regular_theta\n",
    "    return logistic_gradient + regularize_vector\n",
    "\n",
    "init_theta = np.zeros((X.shape[1], 1))\n",
    "lamda = 1\n",
    "\n",
    "# test all-zeros theta and lambda = 1\n",
    "print(regularized_cost(init_theta, lamda, X, y))\n",
    "print(regularized_gradient(init_theta, lamda, X, y))\n",
    "\n",
    "# test all-ones theta and lambda = 10\n",
    "print(regularized_cost(np.ones((X.shape[1], 1)), 10, X, y))\n",
    "print(regularized_gradient(np.ones((X.shape[1], 1)), 10, X, y))\n"
   ]
  },
  {
   "source": [
    "### 2.3.1 Learning parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = opt.minimize(fun = regularized_cost, x0 = init_theta, args = (lamda, X, y.flatten()), method = 'TNC', jac = regularized_gradient)\n",
    "learned_theta = res.x\n",
    "\n",
    "print(res)\n",
    "print(learned_theta)\n",
    "\n",
    "predict_res = predict(learned_theta, X)\n",
    "predict_accuracy = np.sum([1 if predict_res[i] == y[i][0] else 0 for i in range(len(y))]) / len(y)\n",
    "print(\"accuracy on training data: {}\".format(predict_accuracy))"
   ]
  },
  {
   "source": [
    "## 2.4 Plotting the decision boundary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda_list = [0, 1, 10, 100]\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "def power_func(x1, x2, max_power):\n",
    "    res = np.zeros(x1.shape)\n",
    "    power_list = [i for i in range(1, max_power + 1)]\n",
    "    for power in power_list:\n",
    "        x1_powers = [i for i in range(0, power + 1)][::-1]\n",
    "        for x1_power in x1_powers:\n",
    "            x2_power = power - x1_power\n",
    "            res += x1 ** x1_power * x2 ** x2_power\n",
    "    return res\n",
    "\n",
    "\n",
    "for i in range(len(lamda_list)):\n",
    "    lamda = lamda_list[i]\n",
    "    res = opt.minimize(fun = regularized_cost, x0 = init_theta, args = (lamda, X, y.flatten()), method = 'TNC', jac = regularized_gradient)\n",
    "    learned_theta = res.x\n",
    "    x1 = np.linspace(x[:, 0].min(), x[:, 0].max(), 100)\n",
    "    x2 = np.linspace(x[:, 1].min(), x[:, 1].max(), 100)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    y = power_func(X1, X2, 6)\n",
    "    ax = fig.add_subplot(2, 2, i + 1)\n",
    "    ax.set_title(\"lambda = {}\".format(lamda))\n",
    "    ax.contour(x1, x2, y)\n",
    "\n",
    "plt.show()"
   ]
  }
 ]
}