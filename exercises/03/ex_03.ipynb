{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1614243541190",
   "display_name": "Python 3.7.7 64-bit ('ml_course': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Multi-class Classification\n",
    "## 1.1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "from pathlib import Path\n",
    "\n",
    "data_file_1 = Path(os.path.abspath(\"\")) / \"data\" / \"ex3data1.mat\"\n",
    "data_set_1 = sio.loadmat(data_file_1)\n",
    "\n",
    "# every row of X is a flattened 20 * 20 image matrix\n",
    "# every row of y is a digit index, where [1, ..., 9] means digit [1, ..., 9], [10] means digit [0]\n",
    "X = np.array(data_set_1[\"X\"])\n",
    "y = np.array(data_set_1[\"y\"])\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 5, figsize=(8, 8))\n",
    "random_indexes = random.choices([i for i in range(X.shape[0])], k = 25)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X[random_indexes[i]].reshape(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Vectorizing logistic regression\n",
    "### 1.3.1 Vectorizing the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def cost(theta, X, y):\n",
    "    h = sigmoid(X @ theta)\n",
    "    m = X.shape[0]\n",
    "    j = 1 / m * (-y.T @ np.log(h) - (1 - y).T @ np.log(1 - h))\n",
    "    return j.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Vectorizing the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(theta, X, y):\n",
    "    m = X.shape[0]\n",
    "    gradient = 1 / m * X.T @ (sigmoid(X @ theta) - y)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Vectorizing regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def regularized_cost(theta, lamda, X, y):\n",
    "    m = X.shape[0]\n",
    "    regular_theta = np.insert(theta[1:], 0, 0, axis=0)\n",
    "    logistic_cost = cost(theta, X, y)\n",
    "    regularize_factor = lamda / (2 * m) * (regular_theta.T @ regular_theta).item()\n",
    "    return logistic_cost + regularize_factor\n",
    "\n",
    "def regularized_gradient(theta, lamda, X, y):\n",
    "    m = X.shape[0]\n",
    "    regular_theta = np.insert(theta[1:], 0, 0, axis=0)\n",
    "    logistic_gradient = gradient(theta, X, y)\n",
    "    regularize_vector = lamda / m * regular_theta\n",
    "    return logistic_gradient + regularize_vector\n",
    "\n",
    "theta_t = np.array([[-2], [-1], [1], [2]])\n",
    "X_t = np.insert(np.array([i for i in range(1, 16)]).reshape((3, 5)).T / 10, 0, np.ones(5), axis=1)\n",
    "y_t = np.array([[1], [0], [1], [0], [1]])\n",
    "lambda_t = 3\n",
    "print(theta_t)\n",
    "print(X_t)\n",
    "print(y_t)\n",
    "print(lambda_t)\n",
    "\n",
    "cost_t = regularized_cost(theta_t, lambda_t, X_t, y_t)\n",
    "grad_t = regularized_gradient(theta_t, lambda_t, X_t, y_t)\n",
    "print(cost_t)\n",
    "print(grad_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 One-vs-all classication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_labels = 10\n",
    "lamda = 0.1\n",
    "\n",
    "def one_vs_all(X, y, num_labels, lamda):\n",
    "    labels = np.array([i for i in range(1, num_labels + 1)])\n",
    "    y_list = np.array([[1 if i == l else 0 for i in y] for l in labels])\n",
    "    learned_theta_matrix = np.empty((0, X.shape[1] + 1))\n",
    "    for y_ele in y_list:\n",
    "        res = opt.minimize(fun = cost, x0 = np.zeros((X.shape[1] + 1, 1)), args = (np.insert(X, 0, np.ones(X.shape[0]), axis=1), y_ele), method = 'TNC', jac = gradient)\n",
    "        learned_theta = res.x\n",
    "        learned_theta_matrix = np.append(learned_theta_matrix, np.array([learned_theta]), axis=0)\n",
    "    return learned_theta_matrix\n",
    "\n",
    "learned_theta_matrix = one_vs_all(X, y, num_labels, lamda)\n",
    "print(learned_theta_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 One-vs-all prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_one_vs_all(theta_matrix, X):\n",
    "    input_X = np.insert(X, 0, np.ones(X.shape[0]), axis=1)\n",
    "    res_matrix = input_X @ theta_matrix.T\n",
    "    prediction = np.argmax(res_matrix, axis=1) + 1\n",
    "    return prediction\n",
    "\n",
    "pred = predict_one_vs_all(learned_theta_matrix, X)\n",
    "accuracy = np.sum(pred == y.flatten()) / y.shape[0]\n",
    "print(pred)\n",
    "print(y.flatten())\n",
    "print(accuracy)"
   ]
  }
 ]
}