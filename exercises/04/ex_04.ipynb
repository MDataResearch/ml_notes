{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitvenvvenv8825493528554129aa562e16b4229ece",
   "display_name": "Python 3.7.7 64-bit ('.venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "9e6836700e96a853fe4b0f9f6772835270a845f6671132c1dde5eaf8914ce878"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Networks\n",
    "## 1.1 Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "from pathlib import Path\n",
    "\n",
    "data_file_1 = Path(os.path.abspath(\"\")) / \"data\" / \"ex4data1.mat\"\n",
    "data_set_1 = sio.loadmat(data_file_1)\n",
    "\n",
    "# every row of X is a flattened 20 * 20 image matrix\n",
    "# every row of y is a digit index, where [1, ..., 9] means digit [1, ..., 9], [10] means digit [0]\n",
    "X = np.array(data_set_1[\"X\"])\n",
    "y = np.array(data_set_1[\"y\"])\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "# transform y into Y\n",
    "Y = np.zeros((y.shape[0], 10))\n",
    "for i, v in enumerate(y):\n",
    "    Y[i, v - 1] = 1\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Model representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights_file = Path(os.path.abspath(\"\")) / \"data\" / \"ex4weights.mat\"\n",
    "weights = sio.loadmat(weights_file)\n",
    "theta_1 = weights[\"Theta1\"]\n",
    "theta_2 = weights[\"Theta2\"]\n",
    "print(theta_1.shape)\n",
    "print(theta_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Feedforward and cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lamda = 0\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def h(theta_1, theta_2, x):\n",
    "    a_2 = sigmoid(theta_1 @ x)\n",
    "    a_3 = sigmoid(theta_2 @ np.insert(a_2, 0, 1, axis=0))\n",
    "    return a_3\n",
    "\n",
    "def regularization(theta_1, theta_2, lamda, m):\n",
    "    regu_params = np.concatenate((theta_1[:, 1:].flatten(), theta_2[:, 1:].flatten()))\n",
    "    sum = np.sum([i ** 2 for i in regu_params])\n",
    "    res = lamda / (2 * m) * sum\n",
    "    return res\n",
    "\n",
    "def nnCostFunction(theta_1, theta_2, lamda, X, Y):\n",
    "    input_X = np.insert(X, 0, np.ones(X.shape[0]), axis=1)\n",
    "    m = Y.shape[0]\n",
    "    cost = 0\n",
    "    cost_regu = regularization(theta_1, theta_2, lamda, m)\n",
    "    for i in range(m):\n",
    "        x = input_X[i].reshape((input_X.shape[1], 1))\n",
    "        y = Y[i].reshape((Y.shape[1], 1))\n",
    "        H = h(theta_1, theta_2, x)\n",
    "        cost_matrix = 1 / m * (- y * np.log(H) - (1 - y) * np.log(1 - H))\n",
    "        cost += np.sum(cost_matrix)\n",
    "    return cost + cost_regu\n",
    "\n",
    "# cost when lamda = 0\n",
    "print(nnCostFunction(theta_1, theta_2, lamda, X, Y))"
   ]
  },
  {
   "source": [
    "## 1.4 Regularized cost function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cost when lamda = 1\n",
    "print(nnCostFunction(theta_1, theta_2, 1, X, Y))"
   ]
  },
  {
   "source": [
    "# 2. Backpropagation\n",
    "## 2.1 Sigmoid gradient"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "print(sigmoid_gradient(np.array([[0], [0]])))"
   ]
  },
  {
   "source": [
    "## 2.2 Random initialization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epsilon_init = 0.12\n",
    "\n",
    "def rand_initialize_weights(width, height, epsilon):\n",
    "    return np.random.uniform(-epsilon, epsilon, (width, height))\n",
    "\n",
    "init_theta_1 = rand_initialize_weights(theta_1.shape[0], theta_1.shape[1], epsilon_init)\n",
    "init_theta_2 = rand_initialize_weights(theta_2.shape[0], theta_2.shape[1], epsilon_init)\n",
    "print(init_theta_1.shape)\n",
    "print(init_theta_2.shape)"
   ]
  },
  {
   "source": [
    "## 2.3 Backpropagation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate gradient for the theta of each layer\n",
    "def backpropagation(theta_1, theta_2, lamda, X, Y):\n",
    "    input_X = np.insert(X, 0, np.ones(X.shape[0]), axis=1)\n",
    "    m = Y.shape[0]\n",
    "    DELTA_1 = np.zeros(theta_1.shape)\n",
    "    DELTA_2 = np.zeros(theta_2.shape)\n",
    "    for i in range(m):\n",
    "        x = input_X[i].reshape((input_X.shape[1], 1))\n",
    "        y = Y[i].reshape((Y.shape[1], 1))\n",
    "        a_2 = sigmoid(theta_1 @ x)\n",
    "        a_3 = sigmoid(theta_2 @ np.insert(a_2, 0, 1, axis=0))\n",
    "        delta_3 = a_3 - y\n",
    "        delta_2 = (theta_2.T @ delta_3)[1:] * (a_2 * (1 - a_2))\n",
    "        DELTA_2 += delta_3 @ np.insert(a_2, 0, 1, axis=0).T\n",
    "        DELTA_1 += delta_2 @ x.T\n",
    "    gradient_1 = 1 / m * DELTA_1 + lamda / m * np.insert(theta_1[:, 1:], 0, np.zeros(theta_1.shape[0]), axis = 1)\n",
    "    gradient_2 = 1 / m * DELTA_2 + lamda / m * np.insert(theta_2[:, 1:], 0, np.zeros(theta_2.shape[0]), axis = 1)\n",
    "    return gradient_1, gradient_2\n",
    "\n",
    "gradient_1, gradient_2 = backpropagation(theta_1, theta_2, lamda, X, Y)\n",
    "print(gradient_1.shape, gradient_2.shape)"
   ]
  },
  {
   "source": [
    "## 2.4 Gradient checking"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_verification(theta_1, theta_2, lamda, epsilon, X, Y):\n",
    "    gradient_v1 = np.zeros(theta_1.shape)\n",
    "    gradient_v2 = np.zeros(theta_2.shape)\n",
    "    for i in range(gradient_v1.shape[0]):\n",
    "        for j in range(gradient_v1.shape[1]):\n",
    "            theta_1_plus = theta_1.copy()\n",
    "            theta_1_minus = theta_1.copy()\n",
    "            theta_1_plus[i][j] = theta_1_plus[i][j] + epsilon\n",
    "            theta_1_minus[i][j] = theta_1_minus[i][j] - epsilon\n",
    "            gradient_v1[i, j] = (nnCostFunction(theta_1_plus, theta_2, lamda, X, Y) - nnCostFunction(theta_1_minus, theta_2, lamda, X, Y)) / (2 * epsilon)\n",
    "    for i in range(gradient_v2.shape[0]):\n",
    "        for j in range(gradient_v2.shape[1]):\n",
    "            theta_2_plus = theta_2.copy()\n",
    "            theta_2_minus = theta_2.copy()\n",
    "            theta_2_plus[i][j] = theta_2_plus[i][j] + epsilon\n",
    "            theta_2_minus[i][j] = theta_2_minus[i][j] - epsilon\n",
    "            gradient_v2[i, j] = (nnCostFunction(theta_1, theta_2_plus, lamda, X, Y) - nnCostFunction(theta_1, theta_2_minus, lamda, X, Y)) / (2 * epsilon)\n",
    "    return gradient_v1, gradient_v2"
   ]
  },
  {
   "source": [
    "## 2.5 Regularized neural networks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epsilon_v = 1e-4\n",
    "lamda_v = 3\n",
    "\n",
    "# check gradient (heavy calculation)\n",
    "# reduce X, Y to reduce computing time\n",
    "# gradient_1, gradient_2 = backpropagation(theta_1, theta_2, lamda_v, X[:10], Y[:10])\n",
    "# gradient_v1, gradient_v2 = gradient_verification(theta_1, theta_2, lamda_v, epsilon_v, X[:10], Y[:10])\n",
    "# diff_1 = np.absolute(gradient_1 - gradient_v1)\n",
    "# diff_2 = np.absolute(gradient_2 - gradient_v2)\n",
    "# print(np.max(diff_1), np.max(diff_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unrolled_nnCostFunction(unrolled_theta, theta_1_shape, theta_2_shape, lamda, X, Y):\n",
    "    theta_1 = unrolled_theta[:(theta_1_shape[0] * theta_1_shape[1])].reshape(theta_1_shape)\n",
    "    theta_2 = unrolled_theta[(theta_1_shape[0] * theta_1_shape[1]):].reshape(theta_2_shape)\n",
    "    input_X = np.insert(X, 0, np.ones(X.shape[0]), axis=1)\n",
    "    m = Y.shape[0]\n",
    "    cost = 0\n",
    "    cost_regu = regularization(theta_1, theta_2, lamda, m)\n",
    "    for i in range(m):\n",
    "        x = input_X[i].reshape((input_X.shape[1], 1))\n",
    "        y = Y[i].reshape((Y.shape[1], 1))\n",
    "        H = h(theta_1, theta_2, x)\n",
    "        cost_matrix = 1 / m * (- y * np.log(H) - (1 - y) * np.log(1 - H))\n",
    "        cost += np.sum(cost_matrix)\n",
    "    return cost + cost_regu\n",
    "\n",
    "def unrolled_gradient(unrolled_theta, theta_1_shape, theta_2_shape, lamda, X, Y):\n",
    "    theta_1 = unrolled_theta[:(theta_1_shape[0] * theta_1_shape[1])].reshape(theta_1_shape)\n",
    "    theta_2 = unrolled_theta[(theta_1_shape[0] * theta_1_shape[1]):].reshape(theta_2_shape)\n",
    "    input_X = np.insert(X, 0, np.ones(X.shape[0]), axis=1)\n",
    "    m = Y.shape[0]\n",
    "    DELTA_1 = np.zeros(theta_1.shape)\n",
    "    DELTA_2 = np.zeros(theta_2.shape)\n",
    "    for i in range(m):\n",
    "        x = input_X[i].reshape((input_X.shape[1], 1))\n",
    "        y = Y[i].reshape((Y.shape[1], 1))\n",
    "        a_2 = sigmoid(theta_1 @ x)\n",
    "        a_3 = sigmoid(theta_2 @ np.insert(a_2, 0, 1, axis=0))\n",
    "        delta_3 = a_3 - y\n",
    "        delta_2 = (theta_2.T @ delta_3)[1:] * (a_2 * (1 - a_2))\n",
    "        DELTA_2 += delta_3 @ np.insert(a_2, 0, 1, axis=0).T\n",
    "        DELTA_1 += delta_2 @ x.T\n",
    "    gradient_1 = 1 / m * DELTA_1 + lamda / m * np.insert(theta_1[:, 1:], 0, np.zeros(theta_1.shape[0]), axis = 1)\n",
    "    gradient_2 = 1 / m * DELTA_2 + lamda / m * np.insert(theta_2[:, 1:], 0, np.zeros(theta_2.shape[0]), axis = 1)\n",
    "    return np.concatenate((gradient_1.flatten(), gradient_2.flatten()))\n",
    "\n",
    "init_theta_unrolled = np.concatenate((init_theta_1.flatten(), init_theta_2.flatten()))\n",
    "# time-consuming, reduce iteration to computing reduce time\n",
    "res = opt.minimize(fun=unrolled_nnCostFunction, x0=init_theta_unrolled, args=(theta_1.shape, theta_2.shape, lamda, X, Y), method='TNC', jac=unrolled_gradient, options={'maxiter': 100})\n",
    "learned_theta_unrolled = res.x\n",
    "learned_theta_1 = learned_theta_unrolled[:(theta_1.shape[0] * theta_1.shape[1])].reshape(theta_1.shape)\n",
    "learned_theta_2 = learned_theta_unrolled[(theta_1.shape[0] * theta_1.shape[1]):].reshape(theta_2.shape)\n",
    "print(init_theta_unrolled.shape)\n",
    "print(learned_theta_1.shape)\n",
    "print(learned_theta_2.shape)\n",
    "\n",
    "def predict(theta_1, theta_2, X):\n",
    "    input_X = np.insert(X, 0, np.ones(X.shape[0]), axis=1)\n",
    "    a_2 = sigmoid(input_X @ theta_1.T)\n",
    "    a_3 = sigmoid(np.insert(a_2, 0, np.ones(a_2.shape[0]), axis=1) @ theta_2.T)\n",
    "    return a_3\n",
    "\n",
    "random_indexes = random.choices([i for i in range(X.shape[0])], k = 25)\n",
    "X_slice = X[random_indexes]\n",
    "Y_slice = Y[random_indexes]\n",
    "print(X_slice)\n",
    "print(Y_slice)\n",
    "\n",
    "neural_pred = predict(learned_theta_1, learned_theta_2, X_slice)\n",
    "neural_accuracy = np.sum(np.argmax(neural_pred, axis = 1) == np.argmax(Y_slice, axis = 1)) / Y_slice.shape[0]\n",
    "print(neural_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualizing the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show the weights of each pixels that point to each hidden unit \n",
    "fig, ax = plt.subplots(5, 5, figsize=(16,16))\n",
    "r = 0\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        ax[i][j].imshow(learned_theta_1[r][1:].reshape(20, 20))\n",
    "        r += 1\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Optional (ungraded) exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lamdas = [0, 1, 2, 4, 8, 16]\n",
    "max_iters = [25, 50, 100]\n",
    "\n",
    "# try out different lamda and max_iter, compare the results\n",
    "# time-consuming\n",
    "# for lamda in lamdas:\n",
    "#     for max_iter in max_iters:\n",
    "#         res = opt.minimize(fun=unrolled_nnCostFunction, x0=init_theta_unrolled, args=(theta_1.shape, theta_2.shape, lamda, X, Y), method='TNC', jac=unrolled_gradient, options={'maxiter': max_iter})\n",
    "#         learned_theta_unrolled = res.x\n",
    "#         learned_theta_1 = learned_theta_unrolled[:(theta_1.shape[0] * theta_1.shape[1])].reshape(theta_1.shape)\n",
    "#         learned_theta_2 = learned_theta_unrolled[(theta_1.shape[0] * theta_1.shape[1]):].reshape(theta_2.shape)\n",
    "#         neural_pred = predict(learned_theta_1, learned_theta_2, X_slice)\n",
    "#         neural_accuracy = np.sum(np.argmax(neural_pred, axis = 1) == np.argmax(Y_slice, axis = 1)) / Y_slice.shape[0]\n",
    "#         print(\"lamda: {}\".format(lamda))\n",
    "#         print(\"max_iter: {}\".format(max_iter))\n",
    "#         print(\"neural_accuracy: {}\".format(neural_accuracy))"
   ]
  }
 ]
}