{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitvenvvenv3a5d6d845b534a1c8ad6b760f831f4b4",
   "display_name": "Python 3.7.7 64-bit ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regularized Linear Regression\n",
    "## 1.1 visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "data_file = Path(os.path.abspath('')) / 'data' / 'ex5data1.mat'\n",
    "data_set = sio.loadmat(data_file)\n",
    "\n",
    "X = data_set['X']\n",
    "y = data_set['y']\n",
    "X_test = data_set['Xtest']\n",
    "y_test = data_set['ytest']\n",
    "X_val = data_set['Xval']\n",
    "y_val = data_set['yval']\n",
    "\n",
    "print(\"X:{}\".format(X.shape))\n",
    "print(\"y:{}\".format(y.shape))\n",
    "print(\"X_test:{}\".format(X_test.shape))\n",
    "print(\"y_test:{}\".format(y_test.shape))\n",
    "print(\"X_val:{}\".format(X_val.shape))\n",
    "print(\"y_val:{}\".format(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Regularized linear regression cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "theta = np.array([[1], [1]])\n",
    "\n",
    "def linear_regression_cost(theta, lamda, X, y):\n",
    "    input_X = np.insert(X, 0, np.ones(X.shape[0]), 1)\n",
    "    m = y.shape[0]\n",
    "    cost = 1 / (2 * m) * np.sum((input_X @ theta - y) ** 2)\n",
    "    regularization = lamda / (2 * m) * np.sum(theta[1:] ** 2)\n",
    "    return cost + regularization\n",
    "\n",
    "print(linear_regression_cost(theta, 1, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Regularized linear regression gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_regression_gradient(theta, lamda, X, y):\n",
    "    input_X = np.insert(X, 0, np.ones(X.shape[0]), 1)\n",
    "    m = y.shape[0]\n",
    "    theta_zero_gradient = 1 / m * input_X[:, 0:1].T @ (input_X @ theta - y)\n",
    "    theta_rest_gradient = 1 / m * input_X[:, 1:].T @ (input_X @ theta - y) + lamda / m * theta[1:]\n",
    "    gradient = np.insert(theta_rest_gradient, 0, theta_zero_gradient, 0)\n",
    "    return gradient\n",
    "\n",
    "def linear_regression_cost_function(theta, lamda, X, y):\n",
    "    cost = linear_regression_cost(theta, lamda, X, y)\n",
    "    gradient = linear_regression_gradient(theta, lamda, X, y)\n",
    "    return cost, gradient\n",
    "\n",
    "print(linear_regression_cost_function(theta, 1, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Fitting linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lamda = 0\n",
    "\n",
    "res = opt.minimize(fun=linear_regression_cost, x0=theta, args=(lamda, X, y.flatten()), method='TNC', jac=linear_regression_gradient)\n",
    "learned_theta = res.x.reshape((res.x.shape[0], 1))\n",
    "print(learned_theta)\n",
    "\n",
    "x_vector = np.linspace(np.min(X), np.max(X), 100)\n",
    "x_constructed = np.insert(np.array([x_vector]), 0, np.ones(100), axis=0)\n",
    "y_constructed = x_constructed.T @ learned_theta\n",
    "y_vector = y_constructed.flatten()\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(list(X), list(y), marker='o')\n",
    "ax.plot(x_vector, y_vector, \"-\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bias-variance\n",
    "## 2.1 Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = y.shape[0]\n",
    "\n",
    "train_error_array = []\n",
    "val_error_array = []\n",
    "\n",
    "for s in range(1, m + 1):\n",
    "    X_train = X[0:s]\n",
    "    y_train = y[0:s]\n",
    "    res = opt.minimize(fun=linear_regression_cost, x0=theta, args=(lamda, X_train, y_train.flatten()), method='TNC', jac=linear_regression_gradient)\n",
    "    learned_theta = res.x.reshape((res.x.shape[0], 1))\n",
    "    train_error = linear_regression_cost(learned_theta, 0, X_train, y_train)\n",
    "    val_error = linear_regression_cost(learned_theta, 0, X_val, y_val)\n",
    "    train_error_array.append([s, train_error])\n",
    "    val_error_array.append([s, val_error])\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(np.array(train_error_array)[:, 0], np.array(train_error_array)[:, 1], '-', label='training error')\n",
    "ax.plot(np.array(val_error_array)[:, 0], np.array(val_error_array)[:, 1], '-', label='cross validation error')\n",
    "ax.legend(loc=\"upper right\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Polynomial regression\n",
    "## 3.1 Learning Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try lamda = 0, 1, 10, 100\n",
    "lamda = 0\n",
    "p = 8\n",
    "theta_poly = np.ones((p + 1, 1))\n",
    "\n",
    "def map_feature(x, p):\n",
    "    new_x = np.squeeze(np.array([x ** i for i in range(1, p + 1)]).T, axis = 0)\n",
    "    return new_x\n",
    "\n",
    "def featureNormalize(x):\n",
    "    mu = np.mean(x, axis=0).reshape((1, x.shape[1]))\n",
    "    sigma = np.std(x, axis=0).reshape((1, x.shape[1]))\n",
    "    normalized_x = (x - mu) / sigma\n",
    "    return normalized_x, mu, sigma\n",
    "\n",
    "# normalize X\n",
    "X_poly = map_feature(X, p)\n",
    "X_poly_normalized, mu, sigma = featureNormalize(X_poly)\n",
    "\n",
    "# normalize X_val\n",
    "X_poly_val = map_feature(X_val, p)\n",
    "X_poly_normalized_val = (X_poly_val - mu) / sigma\n",
    "\n",
    "# normalize X_test\n",
    "X_poly_test = map_feature(X_test, p)\n",
    "X_poly_normalized_test = (X_poly_test - mu) / sigma\n",
    "\n",
    "# get learned_theta_poly\n",
    "res = opt.minimize(fun=linear_regression_cost, x0=theta_poly, args=(lamda, X_poly_normalized, y.flatten()), method='TNC', jac=linear_regression_gradient)\n",
    "learned_theta_poly = res.x.reshape((res.x.shape[0], 1))\n",
    "print(learned_theta_poly)\n",
    "\n",
    "# prepare plot vectors\n",
    "x_poly_vector = map_feature(x_vector.reshape((x_vector.shape[0], 1)), p)\n",
    "x_poly_vector_normalized = (x_poly_vector - mu) / sigma\n",
    "x_poly_constructed = np.insert(x_poly_vector_normalized, 0, np.ones(x_poly_vector_normalized.shape[0]), axis=1)\n",
    "y_poly_constructed = x_poly_constructed @ learned_theta_poly\n",
    "y_poly_vector = y_poly_constructed.flatten()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(list(X), list(y), marker='o')\n",
    "ax.plot(x_vector, y_poly_vector, \"-\")\n",
    "ax.set_title(\"Figure 1\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_array = []\n",
    "val_error_array = []\n",
    "\n",
    "for s in range(1, m + 1):\n",
    "    X_train = X_poly_normalized[0:s]\n",
    "    y_train = y[0:s]\n",
    "    res = opt.minimize(fun=linear_regression_cost, x0=theta_poly, args=(lamda, X_train, y_train.flatten()), method='TNC', jac=linear_regression_gradient)\n",
    "    learned_theta = res.x.reshape((res.x.shape[0], 1))\n",
    "    train_error = linear_regression_cost(learned_theta, 0, X_train, y_train)\n",
    "    val_error = linear_regression_cost(learned_theta, 0, X_poly_normalized_val, y_val)\n",
    "    train_error_array.append([s, train_error])\n",
    "    val_error_array.append([s, val_error])\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(np.array(train_error_array)[:, 0], np.array(train_error_array)[:, 1], '-', label='training error')\n",
    "ax.plot(np.array(val_error_array)[:, 0], np.array(val_error_array)[:, 1], '-', label='cross validation error')\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_title(\"Figure 2\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "From Figure 1, you should see that the polynomial fit is able to follow the datapoints very well - thus, obtaining a low training error. However, the polynomial fit is very complex and even drops off at the extremes. This is an indicator that the polynomial regression model is overfitting the training data and will not generalize well.\n",
    "\n",
    "To better understand the problems with the unregularized (lamda = 0) model, you can see that the learning curve (Figure 2) shows the same effect where the low training error is low, but the cross validation error is high. There is a gap between the training and cross validation errors, indicating a high variance problem. One way to combat the overfitting (high-variance) problem is to add regularization to the model. In the next section, you will get to try different lamda parameters to see how regularization can lead to a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Adjusting the regularization parameter\n",
    "\n",
    "set lamda = 1, 10, 100 and run 3.1 again\n",
    "\n",
    "For lamda = 1, you should see a polynomial fit that follows the data trend well and a learning curve showing that both the cross validation and training error converge to a relatively low value. This shows the  regularized polynomial regression model does not have the high bias or high-variance problems. In effect, it achieves a good trade-off between bias and variance.\n",
    "\n",
    "For lamda = 100, you should see a polynomial fit that does not follow the data well. In this case, there is too much regularization and the model is unable to fit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Selecting lambda using a cross validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda_list = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "\n",
    "train_error_array = []\n",
    "val_error_array = []\n",
    "\n",
    "for lamda in lamda_list:\n",
    "    res = opt.minimize(fun=linear_regression_cost, x0=theta_poly, args=(lamda, X_poly_normalized, y.flatten()), method='TNC', jac=linear_regression_gradient)\n",
    "    learned_theta = res.x.reshape((res.x.shape[0], 1))\n",
    "    train_error = linear_regression_cost(learned_theta, 0, X_poly_normalized, y)\n",
    "    val_error = linear_regression_cost(learned_theta, 0, X_poly_normalized_val, y_val)\n",
    "    train_error_array.append([lamda, train_error])\n",
    "    val_error_array.append([lamda, val_error])\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(np.array(train_error_array)[:, 0], np.array(train_error_array)[:, 1], '-', label='training error')\n",
    "ax.plot(np.array(val_error_array)[:, 0], np.array(val_error_array)[:, 1], '-', label='cross validation error')\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlabel(\"lamda\")\n",
    "ax.set_ylabel(\"error\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Computing test set error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lamda_best = 3\n",
    "\n",
    "res = opt.minimize(fun=linear_regression_cost, x0=theta_poly, args=(lamda_best, X_poly_normalized, y.flatten()), method='TNC', jac=linear_regression_gradient)\n",
    "learned_theta = res.x.reshape((res.x.shape[0], 1))\n",
    "test_error = linear_regression_cost(learned_theta, 0, X_poly_normalized_test, y_test)\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Plotting learning curves with randomly selected examples\n",
    "\n",
    "In practice, especially for small training sets, when you plot learning curves to debug your algorithms, it is often helpful to average across multiple sets of randomly selected examples to determine the training error and cross validation error. Concretely, to determine the training error and cross validation error for i examples, you should first randomly select i examples from the training set and i examples from the cross validation set. You will then learn the parameters theta using the randomly chosen training set and evaluate the parameters theta on the randomly chosen training set and cross validation set. The above steps should then be repeated multiple times (say 50) and the averaged error should be used to determine the training error and cross validation error for i examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_pick_times = 50\n",
    "lamda = 0.01\n",
    "train_error_array = []\n",
    "val_error_array = []\n",
    "\n",
    "for s in range(1, m + 1):\n",
    "    train_error_array_temp = []\n",
    "    val_error_array_temp = []\n",
    "    for i in range(50):\n",
    "        train_indexes = np.random.choice(X_poly_normalized.shape[0], size=s, replace=False)\n",
    "        val_indexes = np.random.choice(X_poly_normalized_val.shape[0], size=s, replace=False)\n",
    "        X_train_pick = X_poly_normalized[train_indexes]\n",
    "        y_train_pick = y[train_indexes]\n",
    "        X_val_pick = X_poly_normalized_val[val_indexes]\n",
    "        y_val_pick = y_val[val_indexes]\n",
    "        res = opt.minimize(fun=linear_regression_cost, x0=theta_poly, args=(lamda, X_train_pick, y_train_pick.flatten()), method='TNC', jac=linear_regression_gradient)\n",
    "        learned_theta = res.x.reshape((res.x.shape[0], 1))\n",
    "        train_error = linear_regression_cost(learned_theta, 0, X_train_pick, y_train_pick)\n",
    "        val_error = linear_regression_cost(learned_theta, 0, X_val_pick, y_val_pick)\n",
    "        train_error_array_temp.append(train_error)\n",
    "        val_error_array_temp.append(val_error)\n",
    "\n",
    "    train_error_average = np.mean(train_error_array_temp)\n",
    "    val_error_average = np.mean(val_error_array_temp)\n",
    "    train_error_array.append([s, train_error_average])\n",
    "    val_error_array.append([s, val_error_average])\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(np.array(train_error_array)[:, 0], np.array(train_error_array)[:, 1], '-', label='training error')\n",
    "ax.plot(np.array(val_error_array)[:, 0], np.array(val_error_array)[:, 1], '-', label='cross validation error')\n",
    "ax.legend(loc=\"upper right\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ]
}