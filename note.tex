\documentclass{article}
\usepackage{xeCJK}
\usepackage{makeidx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{xcolor}

\title{Machine Learning Notes}
\author{Haotian Chen}
\date{}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
                                     {-3.25ex\@plus -1ex \@minus -.2ex}%
                                     {1.5ex \@plus .2ex}%
                                     {\normalfont\normalsize\bfseries}}
\makeatletter

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=cyan,
}

\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}
\makeindex



\begin{document}

\maketitle

\clearpage

\tableofcontents{}

\clearpage

\section{Machine Learning Introduction}

A computer program is said to learn
from experience E with respect to some task T
and some performance measure P, if its
performance on T, as measured by P, improves
with experience E. 

\subsection{Supervised Learning}

Supervised learning algorithms build a mathematical model 
of a set of data that contains both the inputs and the desired 
outputs.

\bigskip

\noindent In supervised learning, each example is a 
pair consisting of an input object (typically a vector) 
and a desired output value (also called the supervisory signal).

\subsubsection{Regression}

Regression analysis is a set of statistical processes 
for estimating the relationships between a dependent 
variable (often called the `outcome variable') and one 
or more independent variables (often called `predictors', 
`covariates', or `features').

\bigskip

\noindent \textbf{Training(Learning) Process:}

\noindent \textit{observed data(training set)} $\rightarrow$ \textit{learning algorithm} $\rightarrow$ \textit{h(hypothesis)}

\noindent \textit{hypothesis:} 假设

\bigskip

\noindent \textbf{Predicting Process:}

\noindent \textit{independent variable} $\rightarrow$ \textit{h(hypothesis)} $\rightarrow$ \textit{dependent variable}

\paragraph{Linear Regression(from \href{https://en.wikipedia.org/wiki/Linear_regression\#:~:text=In\%20statistics\%2C\%20linear\%20regression\%20is,is\%20called\%20simple\%20linear\%20regression.}{Wikipedia})}

Given a data set \(\{y_i, x_{i1}, ..., x_{ip}\}_{i=1}^n\) of n statistical units, a linear regression model assumes that the relationship between the dependent variable \textbf{y} and the p-vector of regressors \textbf{x} is linear.

\[y_i = \beta_0 + \beta_{1}x_{i1} + \dots + \beta_{p}x_{ip} + \epsilon_i, \: i = 1, \dots, n\]

\[\textbf{y} = X\boldsymbol{\beta} + \boldsymbol{\epsilon}\]

\noindent where

\bigskip

\(
\textbf{y} = 
\begin{bmatrix}
y_1\\
y_2\\
\vdots\\
y_n
\end{bmatrix}
,
X = 
\begin{bmatrix}
\textbf{x}_1^T\\
\textbf{x}_2^T\\
\vdots\\
\textbf{x}_n^T
\end{bmatrix} = 
\begin{bmatrix}
1 & x_{11} & \dots & x_{1p}\\
1 & x_{21} & \dots & x_{2p}\\
\vdots & \vdots & \ddots & \vdots\\
1 & x_{n1} & \dots & x_{np}
\end{bmatrix}
,
\boldsymbol{\beta} = 
\begin{bmatrix}
\beta_0\\
\beta_1\\
\vdots\\
\beta_p
\end{bmatrix}
,
\boldsymbol{\epsilon} = 
\begin{bmatrix}
\epsilon_1\\
\epsilon_2\\
\vdots\\
\epsilon_n
\end{bmatrix}
\)

\bigskip

\noindent \(\textbf{y}\) is a vector of observed values \(y_{i}\ (i=1,\ldots ,n)\) of the variable called the regressand, endogenous variable, response variable, measured variable, criterion variable, or dependent variable.

\bigskip

\noindent \(X\) may be seen as a matrix of row-vectors \(x_{i}\) or of n-dimensional column-vectors \(X_{j}\), which are known as regressors, exogenous variables, explanatory variables, covariates, input variables, predictor variables, or independent variables. Usually a constant is included as one of the regressors. In particular, \(x_{i0} = 1\) for \(i = 1, \dots, n\). The corresponding element of \(\beta\) is called the intercept. 

\bigskip

\noindent \(\boldsymbol{\beta}\) is a \((p + 1)\)-dimensional parameter vector, where \(\beta_{0}\) is the intercept term (if one is included in the model—otherwise \(\boldsymbol{\beta}\) is p-dimensional). Its elements are known as effects or regression coefficients (although the latter term is sometimes reserved for the estimated effects).

\bigskip

\noindent \(\boldsymbol{\epsilon}\) is a vector of values \(\epsilon_{i}\). This part of the model is called the error term, disturbance term, or sometimes noise (in contrast with the ``signal" provided by the rest of the model).

\bigskip

\noindent \textbf{Matrix Concepts}

\bigskip

\noindent identity matrix \(I\)(or \(I_{n \times n}\)):

\[A \cdot I = I \cdot A = A\]

\noindent inverse matrix \(A^{-1}\): If \(A\) is an \(m \times m\) matrix, and if it has an inverse,

\[A \cdot A^{-1} = A^{-1} \cdot A = I\]

\noindent transpose matrix \(A^{T}\): Let \(A\) be an \(m \times n\) matrix. Then \(A^T\) is an \(n \times m\) matrix,

\[A^T_{ij} = A_{ji}\]

\paragraph{Linear Regression Learning (from \href{https://www.coursera.org/learn/machine-learning}{Machine Learning course})}

\noindent For convenience reasons, define \(x_0 = 1\)

\[
\textbf{x} = 
\begin{bmatrix}
x_0\\
x_1\\
\vdots\\
x_n
\end{bmatrix}
,
\boldsymbol{\theta} = 
\begin{bmatrix}
\theta_0\\
\theta_1\\
\vdots\\
\theta_n
\end{bmatrix}
\]

\noindent hypothesis:

\[\textbf{y} = h_{\theta}(\textbf{x}) = \boldsymbol{\theta}^T \textbf{x} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n\]

\noindent training data:

\[(\textbf{x}^{(i)}, \textbf{y}^{(i)})\:for\:i = 1, \dots, m\]

\noindent cost function:

\[J(\boldsymbol{\theta}) = \frac{1}{2m} \sum_{i = 1}^m (h_{\theta}(\textbf{x}^{(i)}) - \textbf{y}^{(i)})^2\]

\noindent goal:

\[\underset{\boldsymbol{\theta}}{\text{minimize}} \: J(\boldsymbol{\theta})\]

\noindent gradient descent:

\noindent repeat until convergence \{
\[\boldsymbol{\theta}_j =: \boldsymbol{\theta}_j - \alpha \frac{\partial}{\partial \boldsymbol{\theta}_j} J(\boldsymbol{\theta}) = \boldsymbol{\theta}_j - \alpha \frac{1}{m} \sum_{i = 1}^m (h_{\theta}(\textbf{x}^{(i)}) - \textbf{y}^{(i)}) \textbf{x}^{(i)}_j\]
\centerline{simultaneously update \(\boldsymbol{\theta}_j\) for \(j = 0, \dots, n\)}
\}

\bigskip

\noindent batch:

\noindent Each step of gradient descent uses all the training examples.

\bigskip

\noindent feature scaling:

\noindent The range of all features should be normalized so that each feature contributes approximately proportionately to the result. Also it helps gradient descent converge much faster.

\noindent mean normalization: (don't apply to \(x_0\))

\[\textbf{x}_i = \frac{\textbf{x}_i - mean(\textbf{x}_i)}{max(\textbf{x}_i) - min(\textbf{x}_i)}\]

\noindent learning rate \(\alpha\):

\noindent If \(\alpha\) is too small: slow convergence.

\noindent If \(\alpha\) is too large: may not decrease on every iteration and thus may not converge.

\noindent try a range of learning rate to find a good one:

\[\alpha = \dots, 0.001, \dots, 0.01, \dots, 0.1, \dots\]

\noindent feature choosing:

\noindent Choose the right features to fit the data set.

\noindent housing price example, choose from:

\[h_{\theta}(x) = \theta_0 + \theta_1 \times frontage + \theta_2 \times depth\]
\[h_{\theta}(x) = \theta_0 + \theta_1 \times area\]

\noindent polynomial regression:

\noindent Use the polynomial model with the machinery of multivariant linear regression.

\noindent housing price example, choose from:

\[h_{\theta}(x) = \theta_0 + \theta_1 \times area + \theta_2 \times area^2\]
\[h_{\theta}(x) = \theta_0 + \theta_1 \times area + \theta_2 \times \sqrt{area}\]

\paragraph{Normal Equation}

\noindent Let:

\[
\textbf{X} = 
\begin{bmatrix}
(\textbf{x}^{(1)})^T\\
(\textbf{x}^{(2)})^T\\
\vdots\\
(\textbf{x}^{(m)})^T
\end{bmatrix}
,
\textbf{Y} = 
\begin{bmatrix}
\textbf{y}^{(1)}\\
\textbf{y}^{(2)}\\
\vdots\\
\textbf{y}^{(m)}
\end{bmatrix}
\]

\noindent Calculate best \(\boldsymbol{\theta}\): \textcolor{orange}{ * math stuff needed for following part}

\[
\boldsymbol{\theta} = (\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{Y}
\]

\noindent If \(\textbf{X}^T\textbf{X}\) is noninvertible, the common causes might be having:

\noindent 1. Redundant features, where two features are very closely related (i.e. they are linearly dependent)

\noindent 2. Too many features (e.g. \(m \leq n\)). In this case, delete some features or use "regularization".

\bigskip

\noindent \textbf{Gradient Descent} vs \textbf{Normal Equation}:

\begin{center}
\begin{tabular}{ | c | c | } 
\hline
\textbf{Gradient Descent} & \textbf{Normal Equation} \\ 
\hline
Need to choose \(\alpha\) & No need to choose \(\alpha\) \\ 
\hline
Needs many iterations & No need to iterate \\ 
\hline
\(O(kn^2)\) & \(O(n^3)\). Need to calculate \((\textbf{X}^T\textbf{X})^{-1}\) \\ 
\hline
Works well when n is large & Slow if n is very large \\ 
\hline
\end{tabular}
\end{center}

\subsubsection{Classification}

Classification is the problem of identifying to which 
of a set of categories (sub-populations) a new observation 
belongs, on the basis of a training set of data containing 
observations (or instances) whose category membership is known.

\subsection{Unsupervised Learning}

Unsupervised learning algorithms take a set of data that 
contains only inputs, and find structure in the data, like 
grouping or clustering of data points.

\bigskip

\noindent Draw inferences from data sets consisting of input 
data without labeled responses.

\subsection{Reinforcement learning}
       
Reinforcement learning is an area of machine learning concerned 
with how software agents ought to take actions in an environment 
so as to maximize some notion of cumulative reward.

\printindex

\end{document}